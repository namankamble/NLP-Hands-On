{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d21f7ff-e3ea-4b7b-972d-86b1f75a551e",
   "metadata": {},
   "source": [
    "***\n",
    "# <center>***Replacing and Correcting Words***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b10ee3-d23f-4e08-9fef-fb8593e384bc",
   "metadata": {},
   "source": [
    "## ***I learned the following natural language processing techniques:***\n",
    "\n",
    "* **Text Normalization:**\n",
    "    * [Stemming words](#stemming) \n",
    "    * [Lemmatizing words with WordNet](#lemmatization)\n",
    "    * [Replacing words matching regular expressions](#regex-replacement)\n",
    "    * [Removing repeating characters](#character-removal)\n",
    "    * [Spelling correction with Enchant](#spelling-correction)\n",
    "    * [Replacing synonyms](#synonym-replacement)\n",
    "    * [Replacing negations with antonyms](#negation-replacement) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163c2b9d-6952-455b-8e2b-77855f6fda8c",
   "metadata": {},
   "source": [
    "***\n",
    "## ***<a id=\"stemming\"></a>Stemming words:***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ecce7e-5a74-4827-98f2-2ded02ef6434",
   "metadata": {},
   "source": [
    "**Stemming** is a technique to remove affixes from a word, ending up with the stem. For example, the stem of cooking is cook, and a good stemming algorithm knows that the ing suffix can be removed. **Stemming** is most commonly used by search engines for indexing \n",
    "words. Instead of storing all forms of a word, a search engine can store only the stems, greatly reducing the size of index while increasing retrieval accuracy.\n",
    "\n",
    "One of the most common stemming algorithms is the **Porter stemming algorithm** by Martin Porter. It is designed to remove and replace well-known suffixes of English words.  NLTK comes with an implementation of the Porter stemming algorithm, which is very easy to use. Simply instantiate the PorterStemmer class and call the stem() method with the word you want to stem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a174e49-0cf8-4ad4-9f32-4a3206cc0f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cook'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('cooking')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91ae099f-cd97-4304-9dbc-8084cca09e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rune'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "stemmer.stem('runing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a7dc3be-67a1-461d-87c8-d7a07c2bc71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'book'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "stemmer.stem('booking')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57b4e819-736a-4a57-a127-0b13cf28ae7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'jump', 'happili', 'histor']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "words = [\"running\", \"jumps\", \"happily\", \"historical\"]\n",
    "# Stemming the words\n",
    "stems = [stemmer.stem(word) for word in words]\n",
    "# Output the stems\n",
    "print(stems)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ed4730-9677-48f4-a419-6cfe60225d98",
   "metadata": {},
   "source": [
    "The `PorterStemmer` class knows a number of regular word forms and suffixes and uses this knowledge to transform your input word to a final stem through a series of steps. The resulting stem is often a shorter word, or at least a common form of the word, which has \n",
    "the same root meaning.\n",
    "\n",
    "There are other stemming algorithms out there besides the Porter stemming algorithm, such as the **Lancaster stemming algorithm**, developed at Lancaster University. NLTK includes it as the LancasterStemmer class.\n",
    "\n",
    "All the stemmers covered next inherit from the StemmerI interface, which defines the stem() method:\n",
    "\n",
    "* **Stemmerl (stem())**\n",
    "  - PorterStemmer()\n",
    "  - RegexpStemmer()\n",
    "  - SnowballStemmer()\n",
    "  - LancasterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bacbe72-70f5-443d-97c4-df1d85cc1d8a",
   "metadata": {},
   "source": [
    "**The LancasterStemmer class:**\n",
    "\n",
    "The functions of the `LancasterStemmer` class are just like the functions of the PorterStemmer class, but can produce slightly different results. It is known to be slightly more aggressive than the PorterStemmer functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "543d5c85-bec2-449d-8bb7-d9c37a096ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cook'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "stemmer.stem('cooking')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75011a1c-b1c3-4fab-9cd1-fa65b8a460dd",
   "metadata": {},
   "source": [
    "**The RegexpStemmer class**\n",
    "\n",
    "It takes a single regular expression (either compiled or as a string) and removes any prefix or suffix that matches the expression.  A `RegexpStemmer` class should only be used in very specific cases that are not covered by the PorterStemmer or the LancasterStemmer class because it can only handle very specific patterns and is not a general-purpose algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2d1e8c0-989e-4853-8426-630220f38410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook\n",
      "cookery\n",
      "leside\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import RegexpStemmer\n",
    "stemmer = RegexpStemmer('ing')\n",
    "print(stemmer.stem('cooking'))\n",
    "print(stemmer.stem('cookery'))\n",
    "print(stemmer.stem('ingleside'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79d5dc34-4b77-4591-8826-8ea6ae460a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cking\n",
      "ckery\n",
      "ingleside\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stemmer = RegexpStemmer('oo')\n",
    "print(stemmer.stem('cooking'))\n",
    "print(stemmer.stem('cookery'))\n",
    "print(stemmer.stem('ingleside'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74476888-0815-496c-a6ae-dc78ec75aa97",
   "metadata": {},
   "source": [
    "**The SnowballStemmer class**\n",
    "\n",
    "The **SnowballStemmer** class supports 13 non-English languages. It also provides two English stemmers: the original porter algorithm as well as the new English stemming algorithm. To use the SnowballStemmer class, create an instance with the name of the language you are using and then call the stem() method. Here is a list of all the supported languages and an example using the Spanish `SnowballStemmer` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f6b3239-d6a8-43b8-a2cb-9b1d30447a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hol'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "languages = SnowballStemmer.languages\n",
    "spanish_stemmer = SnowballStemmer('spanish')\n",
    "spanish_stemmer.stem('hola')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a298b0b-1ed3-4d71-95c5-599989875c26",
   "metadata": {},
   "source": [
    "***\n",
    "## ***<a id=\"lemmatization\"></a>Lemmatizing words with WordNet:*** \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a4a540-14f7-472d-82d6-589a6583b98b",
   "metadata": {},
   "source": [
    "**Lemmatization** is very similar to stemming, but is more akin to synonym replacement. A lemma is a root word, as opposed to the root stem. So unlike stemming, you are always left with a valid word that means the same thing. However, the word you end up with can\n",
    "be completely different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "504169ed-2d10-4a60-a4ec-e562ef0e38e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cooking'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('cooking')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "373605d8-4046-4a54-8a5c-89b7c572b352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cook'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lemmatizer.lemmatize('cooking', pos='v')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc230fff-438b-4230-80c7-8e667c531036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cookbook'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lemmatizer.lemmatize('cookbooks')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da490dea-3811-4142-8eec-7cb075a1e63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cookbook'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lemmatizer.lemmatize('cookbooks')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774e9d3e-4e0d-4175-88ef-f836c025dc2c",
   "metadata": {},
   "source": [
    "The **WordNetLemmatizer** class is a thin wrapper around the wordnet corpus and uses the morphy() function of the **WordNetCorpusReader** class to find a lemma. If no lemma is found, or the word itself is a lemma, the word is returned as is. Unlike with stemming, knowing the part of speech of the word is important. As demonstrated previously, cooking does not return a different lemma unless you specify that the POS is a verb. This is because the default POS is a noun, and as a noun, cooking is its own lemma. On the other hand, cookbooks  is a noun with its singular form, cookbook, as its lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43c6a7d4-66a6-424d-922c-5b1e64d39dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'believ'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('believes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6450b686-612b-486a-98c1-6d845ffeff12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'belief'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lemmatizer.lemmatize('believes')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421dfc3f-6762-41fa-8a84-d1f5433a046e",
   "metadata": {},
   "source": [
    "**Combining stemming with lemmatization**\n",
    "\n",
    "Stemming and lemmatization can be combined to compress words more than either process can by itself. These cases are somewhat rare, but they do exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8a2e2874-218a-498f-a2ad-7af4f3dac0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'buse'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "stemmer.stem('buses')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91eea5ed-8013-4263-bef2-0e69c0031f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bus'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lemmatizer.lemmatize('buses')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "65a3b864-c4fd-4142-8780-7079ce829cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bu'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "stemmer.stem('bus')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7c197d-0d41-4dc6-a512-5ddc3e643b16",
   "metadata": {},
   "source": [
    "***\n",
    "## ***<a id=\"regex-replacement\"></a>Replacing words matching regular expressions:*** \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21a7610-003a-4727-ae3c-06ed16ea9d2c",
   "metadata": {},
   "source": [
    "If **stemming** and **lemmatization** are a kind of linguistic compression, then word replacement can be thought of as error mcorrection or text normalization.\n",
    "    \n",
    "In this phase, we will replace words based on regular expressions, with a focus on expanding contractions. This technique aims to fix this by replacing contractions with their expanded forms, for example, by replacing \"can't\" with \"cannot\" or \"would've\" with \"would have\".\n",
    "\n",
    "First, we need to define a number of replacement patterns. This will be a list of tuple pairs, where the first element is the pattern to match with and the second element is the replacement.\n",
    "\n",
    "then, we will create a RegexpReplacer class that will compile the patterns and provide a replace() method to substitute all the found patterns with their replacements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "61b81c55-f96b-4fd0-af65-b2e850cdcb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "replacement_patterns = [\n",
    "    (r'won\\'t', 'will not'),\n",
    "    (r'can\\'t', 'can not'),\n",
    "    (r'i\\'m', 'i am'),\n",
    "    (r'ain\\'t', 'is not'),\n",
    "    (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "    (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "    (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "    (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "    (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "    (r'(\\w+)\\'d', '\\g<1> would')\n",
    " ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9f9f124d-0340-4d04-aff7-80ebc69e9da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RegexpReplacer(object):\n",
    "    \n",
    "    def __init__(self, patterns=replacement_patterns):\n",
    "        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "    \n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for (pattern, repl) in self.patterns:\n",
    "            s = re.sub(pattern, repl, s)\n",
    "        return s\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "84370a6d-691c-4f3d-b83d-bbd165c2ae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "replacer = RegexpReplacer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0f0590ff-74f2-48c6-9afe-75cb5f8f6e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you can not read it'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "replacer.replace(\"you can't read it\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3b108432-d453-48ee-8659-67d5c1930a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I should have done that thing I did not do'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "replacer.replace(\"I should've done that thing I didn't do\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7c2dbc-dd3d-477b-b802-0db549a6c93d",
   "metadata": {},
   "source": [
    "***\n",
    "## ***<a id=\"character-removal\"></a>Removing repeating characters:*** \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72bd457-3783-4abe-aea2-08e606559719",
   "metadata": {},
   "source": [
    "In casual conversation, people often stray from strict grammar rules. They might write phrases like **I looooooove it** to emphasize their feelings. However, computers don't understand that **looooooove** is just an exaggerated form of **love** unless explicitly instructed. This guide introduces a method to eliminate these repetitive characters, converting them into proper English words.\n",
    "\n",
    "We will create a class modeled after the RegexpReplacer class from the previous guide. This class will feature a **replace()** method that processes a single word and returns a more accurate version by removing excessive repeating characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdf6b859-3cca-4abc-92c6-40b25224fc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'helo', 'yes', 'gose', 'oh']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "class RepeatReplacer(object):\n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "\n",
    "    def replace(self, word):\n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "        if repl_word != word:\n",
    "            return self.replace(repl_word)\n",
    "        else:\n",
    "            return repl_word\n",
    "\n",
    "replacer = RepeatReplacer()\n",
    "words = [\"looooooove\", \"heeeeelllloooooo\", \"yesssssss\", 'goose','oooooooooooooooooooooooooooooh']\n",
    "\n",
    "# Removing repetitive characters\n",
    "corrected_words = [replacer.replace(word) for word in words]\n",
    "\n",
    "print(corrected_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a720e00-8604-4c6e-bbbd-471a34c2e8aa",
   "metadata": {},
   "source": [
    "***\n",
    "## ***<a id=\"spelling-correction\"></a>Spelling correction with Enchant:*** \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2519d83-14a7-4a17-b0ec-e10668f2ec88",
   "metadata": {},
   "source": [
    "Removing repetitive characters is an intense form of spelling correction. In this guide, we will tackle the less drastic task of fixing minor spelling errors using **Enchant** (a spelling correction API).\n",
    "\n",
    "Next, we will create a class named **SpellingReplacer**. This class will feature a **replace()** method that uses Enchant to verify if the word is valid. If the word isn't valid, it will suggest the best alternative using **nltk.metrics.edit_distance()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55d2405a-72e1-4361-aa4b-965518c88c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: I am lernin NLP with corect speling\n",
      "Corrected: I am Lerner NIP with correct spieling\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import enchant\n",
    "\n",
    "# Initialize English dictionary\n",
    "dictionary = enchant.Dict(\"en_US\")\n",
    "\n",
    "# Sample text\n",
    "text = \"I am lernin NLP with corect speling\"\n",
    "\n",
    "# Split the text into words\n",
    "words = text.split()\n",
    "\n",
    "# Correct each word if misspelled\n",
    "corrected_words = []\n",
    "for word in words:\n",
    "    if dictionary.check(word):  # If the word is spelled correctly\n",
    "        corrected_words.append(word)\n",
    "    else:\n",
    "        # Suggest the most probable correction\n",
    "        suggestions = dictionary.suggest(word)\n",
    "        corrected_words.append(suggestions[0] if suggestions else word)  # Use the first suggestion or keep the word\n",
    "\n",
    "# Combine the corrected words into a sentence\n",
    "corrected_text = ' '.join(corrected_words)\n",
    "print(\"Original:\", text)\n",
    "print(\"Corrected:\", corrected_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f5919aa-c5e9-4d58-aeb9-4ba8ee4e8b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "class SpellingReplacer:\n",
    "    def __init__(self, dict_name='en_US', max_dist=2):\n",
    "        self.spell_dict = enchant.Dict(dict_name)\n",
    "        self.max_dist = max_dist\n",
    "\n",
    "    def replace(self, word):\n",
    "        if self.spell_dict.check(word):\n",
    "            return word\n",
    "        suggestions = self.spell_dict.suggest(word)\n",
    "        if suggestions and edit_distance(word, suggestions[0]) <= self.max_dist:\n",
    "            return suggestions[0]\n",
    "        else:\n",
    "            return word\n",
    "\n",
    "# Create an instance of the replacer\n",
    "replacer = SpellingReplacer()\n",
    "\n",
    "# Replace misspelled words\n",
    "print(replacer.replace('helloo'))  # Output: \"cookbook\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c674c5-9afb-4494-8bc0-02b58ddee724",
   "metadata": {},
   "source": [
    "The `SpellingReplacer` class starts by creating a reference to an Enchant dictionary. Then, in the **replace()** method, it first checks whether the given word is present in the dictionary. If it is, no spelling correction is necessary and the word is returned. If the word is not found, it looks up a list of suggestions and returns the first suggestion, as long as its edit distance is less than or equal to max_dist. The edit distance is the number of character changes necessary to transform the given word into the suggested word. The max_dist value then acts as a constraint on the Enchant suggest function to ensure that no unlikely replacement words are returned. Here is an example showing all the suggestions for languege, a misspelling of language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c5168e8-cd63-489d-9f1e-2e8a74672f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['language', 'langue', 'Angela']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "d = enchant.Dict('en')\n",
    "d.suggest('languege')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56ad2f2-24a0-4ab5-b8fe-cb053584a3b5",
   "metadata": {},
   "source": [
    "Except for the correct suggestion, language, all the other words have an edit distance of three or greater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efb320e1-1b9f-48f9-80f9-fc879be2e054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(edit_distance('language', 'languege'))\n",
    "print(edit_distance('language', 'languo'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa12c1e6-1ad9-4f71-ac40-5f00b239f998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['en_BW',\n",
       " 'en_AU',\n",
       " 'en_BZ',\n",
       " 'en_GB',\n",
       " 'en_JM',\n",
       " 'en_DK',\n",
       " 'en_HK',\n",
       " 'en_GH',\n",
       " 'en_US',\n",
       " 'en_ZA',\n",
       " 'en_ZW',\n",
       " 'en_SG',\n",
       " 'en_NZ',\n",
       " 'en_BS',\n",
       " 'en_AG',\n",
       " 'en_PH',\n",
       " 'en_IE',\n",
       " 'en_NA',\n",
       " 'en_TT',\n",
       " 'en_IN',\n",
       " 'en_NG',\n",
       " 'en_CA']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "enchant.list_languages()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63686782-bd97-4185-b2a3-5e9ffc97627c",
   "metadata": {},
   "source": [
    "The `en_US` dictionary can give you different results than `en_GB`, such as for the word theater. The word theater is the American English spelling whereas the British English spelling is theatre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae3ee5d5-1d69-4092-8982-1c32b11f8083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "en_US = enchant.Dict('en_US')\n",
    "print(en_US.check('theater'))\n",
    "\n",
    "en_GB = enchant.Dict('en_GB')\n",
    "print(en_GB.check('theater'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9918a2e-7aa9-4b0c-ba1a-9972b10e9eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theater\n",
      "theatre\n"
     ]
    }
   ],
   "source": [
    "\n",
    "us_replacer = SpellingReplacer('en_US')\n",
    "print(us_replacer.replace('theater'))\n",
    "\n",
    "gb_replacer = SpellingReplacer('en_GB')\n",
    "print(gb_replacer.replace('theater'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbb2825-700b-480e-9cdf-087568e6981b",
   "metadata": {},
   "source": [
    "***\n",
    "## ***<a id=\"synonym-replacement\"></a>Replacing synonyms:*** \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c56d0d-4675-4811-abd5-dba1ebef4c8d",
   "metadata": {},
   "source": [
    "**Synonym replacement** is a common technique in Natural Language Processing (NLP) to augment data, normalize text, or modify input for various tasks. Below is an approach to replacing words with their synonyms.\n",
    "\n",
    "You will need a defined mapping of a word to its synonym. This is a simple controlled vocabulary. We will start by hardcoding the synonyms as a Python dictionary, and then explore other options to store synonym maps.\n",
    "\n",
    "We will first create a `WordReplacer` class that takes a word `replacement mapping`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0af95402-5bef-46b0-b8f0-3412f7d9465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WordReplacer(object):\n",
    "    def __init__(self, word_map):\n",
    "        self.word_map = word_map\n",
    "    def replace(self, word):\n",
    "        return self.word_map.get(word, word)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9ee913-dd23-49f3-b007-f79ba89cc7fe",
   "metadata": {},
   "source": [
    "Then, we can demonstrate its usage for simple word replacement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfbbf28d-79fa-4f79-bac6-c05155519160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'birthday'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "replacer = WordReplacer({'bday': 'birthday'})\n",
    "replacer.replace('bday')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78b170b3-67be-4b11-8de2-6d02713d6b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happy'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "replacer.replace('happy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ebdaa3-88a6-4642-a142-556c79ef0ac7",
   "metadata": {},
   "source": [
    "The `WordReplacer` class is simply a class wrapper around a Python dictionary. The `replace()` method looks up the given word in its **word_map dictionary** and returns the replacement synonym if it exists. Otherwise, the given word is returned as is.\n",
    "\n",
    "If you were only using the **word_map dictionary**, you would not need the WordReplacer class and could instead call **word_map.get()** directly. However, **WordReplacer** can act as a base class for other classes that construct the **word_map dictionary** from various file formats.\n",
    "\n",
    "We can use the **WordReplacer** class to perform any kind of word replacement, even spelling correction for more complicated words that can't be automatically corrected, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d07252-a405-40b2-aaaf-808709dc4e67",
   "metadata": {},
   "source": [
    "***\n",
    "## ***<a id=\"negation-replacement\"></a>Replacing negations with antonyms:***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69df053-bf4f-4edd-91cd-27c896aed6d5",
   "metadata": {},
   "source": [
    "The opposite of synonym replacement is **antonym replacement**. An antonym is a word that has the opposite meaning of another word. This time, instead of creating custom word mappings, we can use **WordNet** to replace words with unambiguous antonyms.\n",
    "\n",
    "Let's say you have a sentence like `let's not uglify our code`. With antonym replacement, you can replace `not uglify` with `beautify`, resulting in the sentence `let's beautify our code`. To do this, we will create an AntonymReplacer class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dfe6f736-857d-4c97-b905-2961391f15cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "class AntonymReplacer:\n",
    "    def replace(self, word, pos=None):\n",
    "        \"\"\"Replace a word with its antonym if one exists.\"\"\"\n",
    "        antonyms = set()\n",
    "        for syn in wordnet.synsets(word, pos=pos):\n",
    "            for lemma in syn.lemmas():\n",
    "                for antonym in lemma.antonyms():\n",
    "                    antonyms.add(antonym.name())\n",
    "        if len(antonyms) == 1:\n",
    "            return antonyms.pop()  # Return the only antonym\n",
    "        else:\n",
    "            return None  # Return None if no antonym or multiple antonyms are found\n",
    "\n",
    "    def replace_negations(self, sent):\n",
    "        \"\"\"\n",
    "        Replace negation phrases (e.g., 'not happy') with their antonyms\n",
    "        (e.g., 'unhappy').\n",
    "        \"\"\"\n",
    "        i, l = 0, len(sent)\n",
    "        words = []\n",
    "        while i < l:\n",
    "            word = sent[i]\n",
    "            if word == 'not' and i + 1 < l:  # Check for 'not' and the next word\n",
    "                ant = self.replace(sent[i + 1])\n",
    "                if ant:\n",
    "                    words.append(ant)  # Replace 'not word' with the antonym\n",
    "                    i += 2  # Skip the next word\n",
    "                    continue\n",
    "            words.append(word)  # Add the word as is\n",
    "            i += 1\n",
    "        return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2b5a8e9-6c98-4d29-a5dc-d3ce43ce0b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"let's\", 'beautify', 'our', 'code']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "replacer = AntonymReplacer()\n",
    "replacer.replace('good')\n",
    "replacer.replace('uglify')\n",
    "sent = [\"let's\", 'not', 'uglify', 'our', 'code']\n",
    "replacer.replace_negations(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7100689-2d7d-475c-acd3-a2b3e014f858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unhappy']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "replacer = AntonymReplacer()\n",
    "sent = ['not', 'happy']\n",
    "replacer.replace_negations(sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20722bb1-b786-4833-bb93-697308b38dc7",
   "metadata": {},
   "source": [
    "The `AntonymReplacer` class has two methods: `replace()` and `replace_negations()`. The `replace()` method takes a single word and an optional part-of-speech tag, then looks up the Synsets for the word in WordNet. Going through all the Synsets and every lemma of each Synset, it creates a set of all antonyms found. If only one antonym is found, then it is an unambiguous replacement. If there is more than one antonym, which can happen quite often, then we don't know for sure which antonym is correct. In the case of multiple antonyms (or no antonyms), replace() returns None as it cannot make a decision.\n",
    "\n",
    "In **replace_negations()**, we look through a tokenized sentence for the word not. If not is found, then we try to find an antonym for the next word using replace(). If we find an antonym, then it is appended to the list of words, replacing not and the original word. All other words are appended as is, resulting in a tokenized sentence with unambiguous negations replaced by their antonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f603fbf-063e-499e-a09e-a2c14479cc4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
