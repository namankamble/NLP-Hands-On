{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b90e7b0d-4727-45ce-b607-38b2ff991ea7",
   "metadata": {},
   "source": [
    "***\n",
    "# <center>***Tokenizing Text and WordNet***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a257ff-0472-420b-b61d-41b90e0d566b",
   "metadata": {},
   "source": [
    "## ***I learned the following natural language processing techniques:***\n",
    "\n",
    "* **Tokenization:**\n",
    "    * [Tokenizing text into sentences](#sentence-tokenization) \n",
    "    * [Tokenizing sentences into words](#word-tokenization)\n",
    "    * [Tokenizing sentences using regular expressions](#regex-tokenization) \n",
    "    * [Training a sentence tokenizer](#training-tokenizer)\n",
    "* **Text Cleaning:**\n",
    "    * [Filtering stopwords in a tokenized sentence](#stop-word-filtering)\n",
    "* **Lexical Semantics:**\n",
    "    * [Synsets for a word in WordNet](#wordnet-synsets) \n",
    "    * [Lemmas and synonyms in WordNet](#wordnet-lemmas-synonyms)\n",
    "    * [Calculating WordNet Synset similarity](#wordnet-similarity)\n",
    "* **Collocation Analysis:** \n",
    "    * [Discovering word collocations](#word-collocations) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77958c4-034d-4445-bd9a-904ee44e040d",
   "metadata": {},
   "source": [
    "***\n",
    "## ***Introduction***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58759fc-06df-43ea-8a70-55b03188c809",
   "metadata": {},
   "source": [
    "**Natural Language ToolKit (NLTK)** is a comprehensive Python library for natural language processing and text analytics. Originally designed for teaching, it has been adopted in the industry for research and development due to its usefulness and breadth of coverage. NLTK is often used for rapid prototyping of text processing programs and can even be used in production applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86624ce8-5a66-46bd-82bc-3805a83293d1",
   "metadata": {},
   "source": [
    "**Tokenization** is a method of breaking up a piece of text into many pieces, such as sentences and words. **WordNet** is a dictionary designed  for programmatic access by natural language processing systems. It has many different use cases, including:\n",
    "- Looking up the definition of a word\n",
    "- Finding synonyms and antonyms\n",
    "- Exploring word relations and similarity\n",
    "- Word sense disambiguation for words that have multiple uses and definitions\n",
    "\n",
    "**NLTK** includes a WordNet corpus reader, which we will use to access and explore WordNet. A corpus is just a body of text, and corpus readers are designed to make accessing a corpus much easier than direct file access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ee1a41-9840-4226-a6ca-b052e188e42d",
   "metadata": {},
   "source": [
    "***\n",
    "### ***<a id=\"sentence-tokenization\"></a>Sentence Tokenization:***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8a690c-90b4-432a-a430-7b960aa1d6aa",
   "metadata": {},
   "source": [
    "**Tokenization** is the process of splitting a string into a list of pieces or tokens. A token is a piece of a whole, so a word is a token in a sentence, and a sentence is a token in a paragraph. We will start with sentence tokenization, or splitting a paragraph into a list of sentences.\n",
    "\n",
    "we can start by creating a paragraph of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "277d59c6-b92d-42d5-a76e-687a99c68866",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "para = \"Hello World. It's good to see you. Thanks forreading this Notebook.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acf76ce-e2b3-4aaf-b320-1420c6c690ef",
   "metadata": {},
   "source": [
    "Now we want to **split the paragraph into sentences**. First we need to import the **sentence tokenization** function, and then we can call it with the paragraph as an argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b91a142b-a17a-4863-b7b9-e8e52a8b2b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello World.', \"It's good to see you.\", 'Thanks forreading this Notebook.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "Sent_tokenize = sent_tokenize(para)\n",
    "Sent_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a989299-a240-444d-97ab-5481c7f7728e",
   "metadata": {},
   "source": [
    "So now we have a list of sentences that we can use for further processing.\n",
    "\n",
    "The sent_tokenize function uses an instance of PunktSentenceTokenizer from the nltk.tokenize.punkt module. This instance has already been trained and works well for many European languages. So it knows what punctuation and characters mark the end of a sentence and the beginning of a new sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1f5e45-efa4-46c4-a792-4d9d5e1ff383",
   "metadata": {},
   "source": [
    "***\n",
    "### **<a id=\"word-tokenization\"></a>Word Tokenization:** \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0187b1d3-8e30-4002-a9c4-eb3182da39a8",
   "metadata": {},
   "source": [
    "In this step, **we will split a sentence into individual words**. The simple task of creating a list of words from a string is an essential part of all text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eed35c6c-c256-4b94-b10a-bace53452837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'World',\n",
       " '.',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'good',\n",
       " 'to',\n",
       " 'see',\n",
       " 'you',\n",
       " '.',\n",
       " 'Thanks',\n",
       " 'forreading',\n",
       " 'this',\n",
       " 'Notebook',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "Word_tokenize = word_tokenize(para)\n",
    "Word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d305da-bca9-4b0e-9d74-fb499286904c",
   "metadata": {},
   "source": [
    "The word_tokenize() function is a wrapper function that calls tokenize() on an instance of the TreebankWordTokenizer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96c5b243-9088-4bc6-9277-3e34f991f499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'World.',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'good',\n",
       " 'to',\n",
       " 'see',\n",
       " 'you.',\n",
       " 'Thanks',\n",
       " 'forreading',\n",
       " 'this',\n",
       " 'Notebook',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "Treebank_word_tokenizer = tokenizer.tokenize(para)\n",
    "Treebank_word_tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb710c9d-c26a-408c-9a12-01d50e287f0f",
   "metadata": {},
   "source": [
    "***more...***\n",
    "\n",
    "Ignoring the obviously named `WhitespaceTokenizer` and `SpaceTokenizer`, there are two other word **tokenizers** worth looking at: `PunktWordTokenizer` and `WordPunctTokenizer.` These differ from `TreebankWordTokenizer` by how they handle punctuation and \n",
    "contractions, but they all inherit from TokenizerI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b377d1-3b83-4c6f-b393-bcb919acfad5",
   "metadata": {},
   "source": [
    "***Separating contractions***\n",
    "- One of the tokenizer's most significant conventions is to separate contractions. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea4f9789-72db-492f-b078-cb273a739807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ca', \"n't\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "word_tokenize(\"Can't\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64dc5d0-724a-4be2-8294-352e016dcef3",
   "metadata": {},
   "source": [
    "***WordPunctTokenizer***\n",
    "- Another alternative word tokenizer is WordPunctTokenizer. It splits all punctuation into separate tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf50baeb-4a7b-4580-8f66-122a2d92079b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Can', \"'\", 't', 'is', 'a', 'contraction', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import WordPunctTokenizer \n",
    "tokenizer = WordPunctTokenizer()\n",
    "Punkt_Word_Tokenizer = tokenizer.tokenize(\"Can't is a contraction.\")\n",
    "print(Punkt_Word_Tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caa6213-3623-4bec-8511-c033e1bcb4c8",
   "metadata": {},
   "source": [
    "***\n",
    "### ***<a id=\"regex-tokenization\"></a>Tokenizing sentences using regular expressions***\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b14b04-4312-4137-a394-ba9f85e6ad56",
   "metadata": {},
   "source": [
    "First you need to decide how you want to tokenize a piece of text as this will determine how you construct your regular expression. The choices are:\n",
    "> Match on the tokens\n",
    "  > \n",
    "> Match on the separators or gaps\n",
    "\n",
    "We will create an instance of RegexpTokenizer, giving it a regular expression string to use for matching tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeea828b-47dd-4580-89e2-21063088daad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'World',\n",
       " \"It's\",\n",
       " 'good',\n",
       " 'to',\n",
       " 'see',\n",
       " 'you',\n",
       " 'Thanks',\n",
       " 'forreading',\n",
       " 'this',\n",
       " 'Notebook']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "Regexp_Tokenizer = tokenizer.tokenize(para)\n",
    "Regexp_Tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1518d947-eda0-45a1-bdd8-72668eb41371",
   "metadata": {},
   "source": [
    "We can also use a simple helper function if you do not want to instantiate the class: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86eaf125-655a-4a0d-980a-0877370a9d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'World',\n",
       " \"It's\",\n",
       " 'good',\n",
       " 'to',\n",
       " 'see',\n",
       " 'you',\n",
       " 'Thanks',\n",
       " 'forreading',\n",
       " 'this',\n",
       " 'Notebook']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "simple_regexp_tokenize = regexp_tokenize(para, \"[\\w']+\")\n",
    "simple_regexp_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4390544b-7d01-475b-9099-d93113d3d000",
   "metadata": {},
   "source": [
    "The **RegexpTokenizer** class works by compiling your pattern, then calling re.findall() on your text. You could do all this yourself using the re module, but RegexpTokenizer implements the TokenizerI interface,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec00867-77bb-4490-ba9e-9215e8a52ee0",
   "metadata": {},
   "source": [
    "***more...***\n",
    " \n",
    "**RegexpTokenizer** can also work by matching the gaps, as opposed to the tokens. Instead of using re.findall(), the RegexpTokenizer class will use re.split(). This is how the `BlanklineTokenizer` class in nltk.tokenize is implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13e7203-266f-46c3-938d-98f0007eaddd",
   "metadata": {},
   "source": [
    "***Simple whitespace tokenizer***\n",
    "- The following is a simple example of using **RegexpTokenizer** to tokenize on whitespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64683e51-4032-4d42-bb66-21d7c3c12d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'World.',\n",
       " \"It's\",\n",
       " 'good',\n",
       " 'to',\n",
       " 'see',\n",
       " 'you.',\n",
       " 'Thanks',\n",
       " 'forreading',\n",
       " 'this',\n",
       " 'Notebook.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer = RegexpTokenizer('\\s+', gaps=True)\n",
    "tokenizer.tokenize(para)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff67a3e-7ac8-4084-9e54-fb57d7e92bd0",
   "metadata": {},
   "source": [
    "***\n",
    "### ***<a id=\"training-tokenizer\"></a>Training a sentence tokenizer***\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52b03a9-4937-4d5b-bfae-d7e14fa94119",
   "metadata": {},
   "source": [
    "**NLTK's** default sentence tokenizer is general purpose, and usually works quite well. But sometimes it is not the best choice for your text. Perhaps your text uses nonstandard punctuation, or is formatted in a unique way. In such cases, training your own sentence \n",
    "tokenizer can result in much more accurate sentence tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17588b4c-1061-49d3-a912-60ea8e444f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import webtext\n",
    "text = webtext.raw('overheard.txt')\n",
    "sent_tokenizer = PunktSentenceTokenizer(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b20e9d81-e300-4742-8a22-5157b43283ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'White guy: So, do you have any plans for this evening?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sents1 = sent_tokenizer.tokenize(text)\n",
    "sents1[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76f87ef9-c6d2-4c46-abd8-e2540e389575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Guy #1: Well, he sort of was, spiritually.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sents2 = sent_tokenize(text)\n",
    "sents2[67]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10371f74-1fa5-419c-accf-073f101a9436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Girl: But you already have a Big Mac...'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sents1[678]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a4f6e51-6354-46c7-89cb-5a89c5f1caf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Girl: But you already have a Big Mac...\\nHobo: Oh, this is all theatrical.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sents2[678]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142349d9-2946-42b2-a717-72669f8c2cd4",
   "metadata": {},
   "source": [
    "While the first sentence is the same, you can see that the tokenizers disagree on how to tokenize sentence 679 (this is the first sentence where the tokenizers diverge). The default tokenizer includes the next line of dialog, while our custom tokenizer correctly thinks that the next line is a separate sentence. This difference is a good demonstration of why it can be useful to train your own sentence tokenizer, especially when your text is not in the typical paragraph sentence structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9db30c-9d04-4082-b46f-700b3dd1fb85",
   "metadata": {},
   "source": [
    "***\n",
    "### ***<a id=\"stop-word-filtering\"></a>Filtering stopwords in a tokenized sentence***\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f1a5f1-f07d-4e06-9e1b-98e7b356472b",
   "metadata": {},
   "source": [
    "**Stopwords** are common words that generally do not contribute to the meaning of a sentence, at least for the purposes of information retrieval and natural language processing. These are words such as ***the and a***. Most search engines will filter out stopwords from search queries and documents in order to save space in their index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8565a430-8a65-4add-be13-4da26e58de8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World. It's good to see you. Thanks forreading this Notebook.\n",
      "Hello World. It's good see you. Thanks forreading Notebook.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "para_1 = para.split()\n",
    "from nltk.corpus import stopwords\n",
    "english_stops = set(stopwords.words('english'))\n",
    "words = para_1\n",
    "Stopwords = [word for word in words if word not in english_stops]\n",
    "print(para)\n",
    "print(\" \".join(Stopwords))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed42cf6-79a8-45b0-b8aa-23ce9afabc56",
   "metadata": {},
   "source": [
    "The **stopwords** corpus is an instance of nltk.corpus.reader. `WordListCorpusReader`. As such, it has a `words()` method that can take a single argument for the file ID, which in this case is **english**, referring to a file containing  a list of **English stopwords**. You could also call **stopwords.words()** with no argument to get a list of all stopwords in every language available.\n",
    "\n",
    "You can see the complete list of languages using the fileids method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dabfa0d9-74ae-459e-b109-f0b49396ea58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'basque',\n",
       " 'bengali',\n",
       " 'catalan',\n",
       " 'chinese',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hebrew',\n",
       " 'hinglish',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "stopwords.fileids()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96802d20-40bc-4262-863c-aae4d3c93954",
   "metadata": {},
   "source": [
    "Any of these fileids can be used as an argument to the words() method to get a list of stopwords for that language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "94ad49b9-4731-4f8c-8549-ee5c8dc7aee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "63105329-25b7-44e7-ad37-3d1c7414d893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de',\n",
       " 'en',\n",
       " 'van',\n",
       " 'ik',\n",
       " 'te',\n",
       " 'dat',\n",
       " 'die',\n",
       " 'in',\n",
       " 'een',\n",
       " 'hij',\n",
       " 'het',\n",
       " 'niet',\n",
       " 'zijn',\n",
       " 'is',\n",
       " 'was',\n",
       " 'op',\n",
       " 'aan',\n",
       " 'met',\n",
       " 'als',\n",
       " 'voor',\n",
       " 'had',\n",
       " 'er',\n",
       " 'maar',\n",
       " 'om',\n",
       " 'hem',\n",
       " 'dan',\n",
       " 'zou',\n",
       " 'of',\n",
       " 'wat',\n",
       " 'mijn',\n",
       " 'men',\n",
       " 'dit',\n",
       " 'zo',\n",
       " 'door',\n",
       " 'over',\n",
       " 'ze',\n",
       " 'zich',\n",
       " 'bij',\n",
       " 'ook',\n",
       " 'tot',\n",
       " 'je',\n",
       " 'mij',\n",
       " 'uit',\n",
       " 'der',\n",
       " 'daar',\n",
       " 'haar',\n",
       " 'naar',\n",
       " 'heb',\n",
       " 'hoe',\n",
       " 'heeft',\n",
       " 'hebben',\n",
       " 'deze',\n",
       " 'u',\n",
       " 'want',\n",
       " 'nog',\n",
       " 'zal',\n",
       " 'me',\n",
       " 'zij',\n",
       " 'nu',\n",
       " 'ge',\n",
       " 'geen',\n",
       " 'omdat',\n",
       " 'iets',\n",
       " 'worden',\n",
       " 'toch',\n",
       " 'al',\n",
       " 'waren',\n",
       " 'veel',\n",
       " 'meer',\n",
       " 'doen',\n",
       " 'toen',\n",
       " 'moet',\n",
       " 'ben',\n",
       " 'zonder',\n",
       " 'kan',\n",
       " 'hun',\n",
       " 'dus',\n",
       " 'alles',\n",
       " 'onder',\n",
       " 'ja',\n",
       " 'eens',\n",
       " 'hier',\n",
       " 'wie',\n",
       " 'werd',\n",
       " 'altijd',\n",
       " 'doch',\n",
       " 'wordt',\n",
       " 'wezen',\n",
       " 'kunnen',\n",
       " 'ons',\n",
       " 'zelf',\n",
       " 'tegen',\n",
       " 'na',\n",
       " 'reeds',\n",
       " 'wil',\n",
       " 'kon',\n",
       " 'niets',\n",
       " 'uw',\n",
       " 'iemand',\n",
       " 'geweest',\n",
       " 'andere']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "stopwords.words('dutch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "081c74a7-8bd5-4ccb-baa9-ab5062cffc91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['αλλα',\n",
       " 'αν',\n",
       " 'αντι',\n",
       " 'απο',\n",
       " 'αυτα',\n",
       " 'αυτεσ',\n",
       " 'αυτη',\n",
       " 'αυτο',\n",
       " 'αυτοι',\n",
       " 'αυτοσ',\n",
       " 'αυτουσ',\n",
       " 'αυτων',\n",
       " 'αἱ',\n",
       " 'αἳ',\n",
       " 'αἵ',\n",
       " 'αὐτόσ',\n",
       " 'αὐτὸς',\n",
       " 'αὖ',\n",
       " 'γάρ',\n",
       " 'γα',\n",
       " 'γα^',\n",
       " 'γε',\n",
       " 'για',\n",
       " 'γοῦν',\n",
       " 'γὰρ',\n",
       " \"δ'\",\n",
       " 'δέ',\n",
       " 'δή',\n",
       " 'δαί',\n",
       " 'δαίσ',\n",
       " 'δαὶ',\n",
       " 'δαὶς',\n",
       " 'δε',\n",
       " 'δεν',\n",
       " \"δι'\",\n",
       " 'διά',\n",
       " 'διὰ',\n",
       " 'δὲ',\n",
       " 'δὴ',\n",
       " 'δ’',\n",
       " 'εαν',\n",
       " 'ειμαι',\n",
       " 'ειμαστε',\n",
       " 'ειναι',\n",
       " 'εισαι',\n",
       " 'ειστε',\n",
       " 'εκεινα',\n",
       " 'εκεινεσ',\n",
       " 'εκεινη',\n",
       " 'εκεινο',\n",
       " 'εκεινοι',\n",
       " 'εκεινοσ',\n",
       " 'εκεινουσ',\n",
       " 'εκεινων',\n",
       " 'ενω',\n",
       " 'επ',\n",
       " 'επι',\n",
       " 'εἰ',\n",
       " 'εἰμί',\n",
       " 'εἰμὶ',\n",
       " 'εἰς',\n",
       " 'εἰσ',\n",
       " 'εἴ',\n",
       " 'εἴμι',\n",
       " 'εἴτε',\n",
       " 'η',\n",
       " 'θα',\n",
       " 'ισωσ',\n",
       " 'κ',\n",
       " 'καί',\n",
       " 'καίτοι',\n",
       " 'καθ',\n",
       " 'και',\n",
       " 'κατ',\n",
       " 'κατά',\n",
       " 'κατα',\n",
       " 'κατὰ',\n",
       " 'καὶ',\n",
       " 'κι',\n",
       " 'κἀν',\n",
       " 'κἂν',\n",
       " 'μέν',\n",
       " 'μή',\n",
       " 'μήτε',\n",
       " 'μα',\n",
       " 'με',\n",
       " 'μεθ',\n",
       " 'μετ',\n",
       " 'μετά',\n",
       " 'μετα',\n",
       " 'μετὰ',\n",
       " 'μη',\n",
       " 'μην',\n",
       " 'μἐν',\n",
       " 'μὲν',\n",
       " 'μὴ',\n",
       " 'μὴν',\n",
       " 'να',\n",
       " 'ο',\n",
       " 'οι',\n",
       " 'ομωσ',\n",
       " 'οπωσ',\n",
       " 'οσο',\n",
       " 'οτι',\n",
       " 'οἱ',\n",
       " 'οἳ',\n",
       " 'οἷς',\n",
       " 'οὐ',\n",
       " 'οὐδ',\n",
       " 'οὐδέ',\n",
       " 'οὐδείσ',\n",
       " 'οὐδεὶς',\n",
       " 'οὐδὲ',\n",
       " 'οὐδὲν',\n",
       " 'οὐκ',\n",
       " 'οὐχ',\n",
       " 'οὐχὶ',\n",
       " 'οὓς',\n",
       " 'οὔτε',\n",
       " 'οὕτω',\n",
       " 'οὕτως',\n",
       " 'οὕτωσ',\n",
       " 'οὖν',\n",
       " 'οὗ',\n",
       " 'οὗτος',\n",
       " 'οὗτοσ',\n",
       " 'παρ',\n",
       " 'παρά',\n",
       " 'παρα',\n",
       " 'παρὰ',\n",
       " 'περί',\n",
       " 'περὶ',\n",
       " 'ποια',\n",
       " 'ποιεσ',\n",
       " 'ποιο',\n",
       " 'ποιοι',\n",
       " 'ποιοσ',\n",
       " 'ποιουσ',\n",
       " 'ποιων',\n",
       " 'ποτε',\n",
       " 'που',\n",
       " 'ποῦ',\n",
       " 'προ',\n",
       " 'προσ',\n",
       " 'πρόσ',\n",
       " 'πρὸ',\n",
       " 'πρὸς',\n",
       " 'πως',\n",
       " 'πωσ',\n",
       " 'σε',\n",
       " 'στη',\n",
       " 'στην',\n",
       " 'στο',\n",
       " 'στον',\n",
       " 'σόσ',\n",
       " 'σύ',\n",
       " 'σύν',\n",
       " 'σὸς',\n",
       " 'σὺ',\n",
       " 'σὺν',\n",
       " 'τά',\n",
       " 'τήν',\n",
       " 'τί',\n",
       " 'τίς',\n",
       " 'τίσ',\n",
       " 'τα',\n",
       " 'ταῖς',\n",
       " 'τε',\n",
       " 'την',\n",
       " 'τησ',\n",
       " 'τι',\n",
       " 'τινα',\n",
       " 'τις',\n",
       " 'τισ',\n",
       " 'το',\n",
       " 'τοί',\n",
       " 'τοι',\n",
       " 'τοιοῦτος',\n",
       " 'τοιοῦτοσ',\n",
       " 'τον',\n",
       " 'τοτε',\n",
       " 'του',\n",
       " 'τούσ',\n",
       " 'τοὺς',\n",
       " 'τοῖς',\n",
       " 'τοῦ',\n",
       " 'των',\n",
       " 'τό',\n",
       " 'τόν',\n",
       " 'τότε',\n",
       " 'τὰ',\n",
       " 'τὰς',\n",
       " 'τὴν',\n",
       " 'τὸ',\n",
       " 'τὸν',\n",
       " 'τῆς',\n",
       " 'τῆσ',\n",
       " 'τῇ',\n",
       " 'τῶν',\n",
       " 'τῷ',\n",
       " 'ωσ',\n",
       " \"ἀλλ'\",\n",
       " 'ἀλλά',\n",
       " 'ἀλλὰ',\n",
       " 'ἀλλ’',\n",
       " 'ἀπ',\n",
       " 'ἀπό',\n",
       " 'ἀπὸ',\n",
       " 'ἀφ',\n",
       " 'ἂν',\n",
       " 'ἃ',\n",
       " 'ἄλλος',\n",
       " 'ἄλλοσ',\n",
       " 'ἄν',\n",
       " 'ἄρα',\n",
       " 'ἅμα',\n",
       " 'ἐάν',\n",
       " 'ἐγώ',\n",
       " 'ἐγὼ',\n",
       " 'ἐκ',\n",
       " 'ἐμόσ',\n",
       " 'ἐμὸς',\n",
       " 'ἐν',\n",
       " 'ἐξ',\n",
       " 'ἐπί',\n",
       " 'ἐπεὶ',\n",
       " 'ἐπὶ',\n",
       " 'ἐστι',\n",
       " 'ἐφ',\n",
       " 'ἐὰν',\n",
       " 'ἑαυτοῦ',\n",
       " 'ἔτι',\n",
       " 'ἡ',\n",
       " 'ἢ',\n",
       " 'ἣ',\n",
       " 'ἤ',\n",
       " 'ἥ',\n",
       " 'ἧς',\n",
       " 'ἵνα',\n",
       " 'ὁ',\n",
       " 'ὃ',\n",
       " 'ὃν',\n",
       " 'ὃς',\n",
       " 'ὅ',\n",
       " 'ὅδε',\n",
       " 'ὅθεν',\n",
       " 'ὅπερ',\n",
       " 'ὅς',\n",
       " 'ὅσ',\n",
       " 'ὅστις',\n",
       " 'ὅστισ',\n",
       " 'ὅτε',\n",
       " 'ὅτι',\n",
       " 'ὑμόσ',\n",
       " 'ὑπ',\n",
       " 'ὑπέρ',\n",
       " 'ὑπό',\n",
       " 'ὑπὲρ',\n",
       " 'ὑπὸ',\n",
       " 'ὡς',\n",
       " 'ὡσ',\n",
       " 'ὥς',\n",
       " 'ὥστε',\n",
       " 'ὦ',\n",
       " 'ᾧ']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "stopwords.words('greek')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e61e60-ca3e-493e-ae85-b72dbc3126c5",
   "metadata": {},
   "source": [
    "***\n",
    "### ***<a id=\"wordnet-synsets\"></a>Synsets for a word in WordNet***\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2f3054-229a-4687-897a-a36d0500bb01",
   "metadata": {},
   "source": [
    "**WordNet** is a ***lexical database*** for the English language. In other words, it's a dictionary designed specifically for natural language processing.\n",
    "\n",
    "**NLTK** comes with a simple interface to look up words in **WordNet**. What you get is a list of Synset instances, which are groupings of synonymous words that express the same concept. Many words have only one Synset, but some have several. \n",
    "\n",
    "Synset for *cookbook*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "694c43d8-4d5f-4cd4-ae53-b6bd27722e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cookbook.n.01'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import wordnet\n",
    "syn = wordnet.synsets('cookbook')[0]\n",
    "syn.name()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d44031d6-bc9b-45f5-b10a-8e06814d5f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a book of recipes and cooking directions'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "syn.definition()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d206dcb-1ef2-4cb3-a578-438812b75905",
   "metadata": {},
   "source": [
    "Any word in **WordNet** using ***wordnet.synsets(word)*** to get a list of Synsets. The list may be empty if the word is not found. The list may also have quite a few elements, as some words can have many possible meanings, and, therefore, many Synsets.\n",
    "\n",
    "Each **Synset** in the list has a number of methods you can use to learn more about it. The **name()** method will give you a unique name for the Synset, which you can use to get the Synset directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0cd784c6-b582-4e73-90b5-c83661706ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('cookbook.n.01')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "wordnet.synset('cookbook.n.01')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bd5848-98ae-4575-8052-3c587c7302a9",
   "metadata": {},
   "source": [
    "The `definition()` method should be self-explanatory. Some Synsets also have an `examples()` method, which contains a list of phrases that use the word in context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "62e7c29d-6a27-432e-83b5-7106b55031e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cooking can be a great art',\n",
       " 'people are needed who have experience in cookery',\n",
       " 'he left the preparation of meals to his wife']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "wordnet.synsets('cooking')[0].examples()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e28115-60d1-4b1e-9048-c662f822754b",
   "metadata": {},
   "source": [
    "***\n",
    "#### ***Working with hypernyms***\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f64de38-c368-48a5-b08b-d9bdbba28291",
   "metadata": {},
   "source": [
    "Synsets are organized in a structure similar to that of an inheritance tree. More abstract terms are known as **hypernyms** and more specific terms are **hyponyms**. This tree can be traced all the way up to a root hypernym. Hypernyms provide a way to categorize and group words based on their similarity to each other. The Calculating WordNet Synset similarity recipe details the functions used to calculate the similarity based on the distance between two words in the hypernym tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "79eb9c39-848a-4f95-9a26-3ffd2439da1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('reference_book.n.01')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "syn.hypernyms()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ace08623-4be0-4f5e-9e71-c9d468adc6c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('annual.n.02'),\n",
       " Synset('atlas.n.02'),\n",
       " Synset('cookbook.n.01'),\n",
       " Synset('directory.n.01'),\n",
       " Synset('encyclopedia.n.01'),\n",
       " Synset('handbook.n.01'),\n",
       " Synset('instruction_book.n.01'),\n",
       " Synset('source_book.n.01'),\n",
       " Synset('wordbook.n.01')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "syn.hypernyms()[0].hyponyms()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e4604a18-efb9-45fd-ac05-2af9b166b55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "syn.root_hypernyms()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908b0648-b6bd-4d1d-b540-b7a15a507ff7",
   "metadata": {},
   "source": [
    "As you can see, *reference_book* is a **hypernym** of *cookbook*, but cookbook is only one of the many hyponyms of *reference_book*. And all these types of books have the same root **hypernym**, which is entity, one of the most abstract terms in the English language. You can trace the entire path from entity down to cookbook using the **hypernym_paths()** method, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "995cbcbd-755c-492d-9e15-b4f2a53a4f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Synset('entity.n.01'),\n",
       "  Synset('physical_entity.n.01'),\n",
       "  Synset('object.n.01'),\n",
       "  Synset('whole.n.02'),\n",
       "  Synset('artifact.n.01'),\n",
       "  Synset('creation.n.02'),\n",
       "  Synset('product.n.02'),\n",
       "  Synset('work.n.02'),\n",
       "  Synset('publication.n.01'),\n",
       "  Synset('book.n.01'),\n",
       "  Synset('reference_book.n.01'),\n",
       "  Synset('cookbook.n.01')]]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "syn.hypernym_paths()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c8555f-de14-4bf9-b9d4-936972a8314f",
   "metadata": {},
   "source": [
    "The `hypernym_paths()` method returns a list of lists, where each list starts at the roothypernym and ends with the original Synset. Most of the time, you'll only get one nested  list of Synsets.\n",
    "\n",
    "You can also look up a simplified **part-of-speech** tag as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ebffc6fa-bd8f-4e80-9455-aaa6f288aef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "syn.pos()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72628eef-933c-44d6-9602-9d74236c1856",
   "metadata": {},
   "source": [
    "There are four common part-of-speech tags (or POS tags) found in WordNet, as shown in the following table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c7b8b63a-e720-4b9c-a78e-6ca3cdd85cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Part of speech</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Noun</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adjective</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adverb</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Verb</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Part of speech Tag\n",
       "0           Noun   n\n",
       "1      Adjective   a\n",
       "2         Adverb   r\n",
       "3           Verb   v"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pos = {'Part of speech': ['Noun', 'Adjective', 'Adverb','Verb'], 'Tag': ['n', 'a', 'r', 'v'] }\n",
    "import pandas as pd\n",
    "pos_dataframe = pd.DataFrame(pos)\n",
    "pos_dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bf6fa3-ea58-47c6-8084-4219fa9d24ed",
   "metadata": {},
   "source": [
    "These **POS** tags can be used to look up specific Synsets for a word. For example, the word ***great*** can be used as a noun or an adjective. In WordNet, ***great*** has 1 noun Synset and 6 adjective Synsets, as shown in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "286a9f3a-0c0c-4758-ba66-775f16722838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(wordnet.synsets('great'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7c1ba480-179a-4bcc-88c8-e80008b9f2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(wordnet.synsets('great', pos='n'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3e2a305d-4c1a-495c-a4e7-3dd1ec80d852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(wordnet.synsets('great', pos='a'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e648f90-a24d-43d5-a603-0ca16aab4fd6",
   "metadata": {},
   "source": [
    "***\n",
    "### ***<a id=\"wordnet-lemmas-synonyms\"></a>lemmas and synonyms in WordNet***\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69b3cf1-3f45-42e2-9768-b52c300e3b13",
   "metadata": {},
   "source": [
    "we can also luse lemmas in WordNet to find synonyms of a word. A **lemma (in linguistics)**, is the canonical form or morphological form of a word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e66a399-0ad2-49b2-9928-75272729aa97",
   "metadata": {},
   "source": [
    "In the following code, we will find that there are two lemmas for the cookbook Synset using the **lemmas()** method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6ab599d5-9f89-4e59-8c35-1da24fc0a6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import wordnet\n",
    "syn = wordnet.synsets('cookbook')[0]\n",
    "lemmas = syn.lemmas()\n",
    "len(lemmas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ab84302c-fd08-4008-909c-1ac6f959b65e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cookbook'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lemmas[0].name()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6d32de89-4c94-4da1-8e4d-41faa87fbebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cookery_book'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lemmas[1].name()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "679594eb-cc12-41f1-ad74-9d579fb35276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lemmas[0].synset() == lemmas[1].synset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6e8e1d-dea5-4b67-948a-1a54d0768917",
   "metadata": {},
   "source": [
    "As you can see, cookery_book and cookbook are two distinct lemmas in the same Synset. In fact, a lemma can only belong to a single Synset. In this way, a Synset represents a group  of lemmas that all have the same meaning, while a lemma represents a distinct word form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162ae874-3761-4588-ad4b-a5c60d79d187",
   "metadata": {},
   "source": [
    "***\n",
    "#### ***<a id=\"wordnet-similarity\"></a>Calculating WordNet Synset similarity***\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cddcf2-5a91-4bfe-bc44-4e918a8fc741",
   "metadata": {},
   "source": [
    "***Synsets*** are organized in a hypernym tree. This tree can be used for reasoning about the similarity between the Synsets it contains. The closer the two Synsets are in the tree, the more similar they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2d0c43e1-a4e0-46ed-958f-3ac83e8b8864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import wordnet\n",
    "cb = wordnet.synset('cookbook.n.01')\n",
    "ib = wordnet.synset('instruction_book.n.01')\n",
    "cb.wup_similarity(ib)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95f0ad0-64d2-42a6-9a9b-2b43e25e24d0",
   "metadata": {},
   "source": [
    "The ``wup_similarity``method is short for **Wu-Palmer Similarity**, which is a scoring method based on how similar the word senses are and where the Synsets occur relative to each other in the hypernym tree. One of the core metrics used to calculate similarity is the shortest path distance between the two Synsets and their common hypernym:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d5d74f3b-2e06-4782-97cb-04e9caaa40e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ref = cb.hypernyms()[0]\n",
    "cb.shortest_path_distance(ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ad70c9db-9091-4078-8199-c026c804566e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ib.shortest_path_distance(ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b29dc66a-858d-4370-9f5f-4335e2b834fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cb.shortest_path_distance(ib)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac83bec-e55b-4505-a829-046b710e6248",
   "metadata": {},
   "source": [
    "***\n",
    "### ***<a id=\"word-collocations\"></a>Discovering word collocations***\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f746f0-0cd9-4a84-b46a-d5df4d53d95d",
   "metadata": {},
   "source": [
    "**Collocations** are two or more words that tend to appear frequently together, such as United States. Of course, there are many other words that can come after United, such as United Kingdom and United Airlines. As with many aspects of natural language processing, context  is very important. And for collocations, context is everything!\n",
    "\n",
    "In the case of collocations, the context will be a document in the form of a list of words. Discovering collocations in this list of words means that we'll find common phrases that  occur frequently throughout the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6066666-723e-46c6-b986-20c7bf92758b",
   "metadata": {},
   "source": [
    "We are creating a list of all lowercased words in the text, and then produce `BigramCollocationFinder`, which we can use to find bigrams, which are pairs of words. These bigrams are found using association measurement functions in the **nltk.metrics** package, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "10916ffc-313b-402e-a059-6e9f2373a08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'\", 's'),\n",
       " ('arthur', ':'),\n",
       " ('#', '1'),\n",
       " (\"'\", 't'),\n",
       " ('villager', '#'),\n",
       " ('#', '2'),\n",
       " (']', '['),\n",
       " ('1', ':')]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import webtext\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "words = [w.lower() for w in webtext.words('grail.txt')]\n",
    "bcf = BigramCollocationFinder.from_words(words)\n",
    "bcf.nbest(BigramAssocMeasures.likelihood_ratio, 8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d896a660-8bbe-46de-83f7-72aa00420c40",
   "metadata": {},
   "source": [
    "that's not very useful! Let's refine it a bit by adding a word filter to remove punctuation and stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9163b43e-dccf-4a87-96cc-bc50b0edb406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('black', 'knight'),\n",
       " ('clop', 'clop'),\n",
       " ('head', 'knight'),\n",
       " ('mumble', 'mumble'),\n",
       " ('squeak', 'squeak'),\n",
       " ('saw', 'saw'),\n",
       " ('holy', 'grail'),\n",
       " ('run', 'away')]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopset = set(stopwords.words('english'))\n",
    "filter_stops = lambda w: len(w) < 3 or w in stopset\n",
    "bcf.apply_word_filter(filter_stops)\n",
    "bcf.nbest(BigramAssocMeasures.likelihood_ratio, 8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc32ef9-da8e-4935-95c5-f2d54475b052",
   "metadata": {},
   "source": [
    "`BigramCollocationFinder` constructs two frequency distributions: one for each word, and another for bigrams. A frequency distribution, or FreqDist in NLTK, is basically an enhanced Python dictionary where the keys are what's being counted, and the values are the counts. Any filtering functions that are applied reduce the size of these two FreqDists by eliminating any words that don't pass the filter. By using a filtering function to eliminate all words that are one or two characters, and all English stopwords, we can get a much cleaner result. After filtering, the collocation finder is ready to accept a generic scoring function for finding collocations.\n",
    "\n",
    "In addition to BigramCollocationFinder, there's also TrigramCollocationFinder, which finds triplets instead of pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5c69b9fa-6f44-40a7-86fc-f654cbaa77a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('long', 'term', 'relationship')]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "words = [w.lower() for w in webtext.words('singles.txt')]\n",
    "tcf = TrigramCollocationFinder.from_words(words)\n",
    "tcf.apply_word_filter(filter_stops)\n",
    "tcf.apply_freq_filter(3)\n",
    "tcf.nbest(TrigramAssocMeasures.likelihood_ratio, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa339fb-a2f3-4834-976b-5b74a7fad128",
   "metadata": {},
   "source": [
    "In addition to the stopword filter, I also applied a frequency filter, which removed any trigrams that occurred less than three times. This is why only one result was returned when we asked for four because there was only one result that occurred more than two times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85e937b-9fcd-43b7-8e34-5f6c2f5d7422",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
