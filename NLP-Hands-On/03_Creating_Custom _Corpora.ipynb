{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef468966-4839-4243-b244-d92dd2d7944b",
   "metadata": {},
   "source": [
    "***\n",
    "# <center>***Creating Custom Corpora***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cee056a-0b6f-42db-b92d-62732549d354",
   "metadata": {},
   "source": [
    "## ***I learned the following natural language processing techniques:***\n",
    "\n",
    "* [Setting up a custom corpus](#custom-corpus)\n",
    "* [Creating a wordlist corpus](#wordlist-corpus)\n",
    "* [Creating a part-of-speech tagged word corpus](#pos-tagged-corpus)\n",
    "* [Creating a chunked phrase corpus](#chunked-corpus)\n",
    "* [Creating a categorized text corpus](#categorized-text-corpus)\n",
    "* [Creating a categorized chunk corpus reader](#categorized-chunk-corpus)\n",
    "* [Lazy corpus loading](#lazy-loading)\n",
    "* [Creating a custom corpus view](#custom-corpus-view)\n",
    "* [Creating a MongoDB-backed corpus reader](#mongodb-corpus)\n",
    "* [Corpus editing with file locking](#file-locking)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f5ff4f-b1b2-494b-b6fc-d074dea1db7d",
   "metadata": {},
   "source": [
    "In this notebook, I have covered how to use **corpus** readers and create custom corpora. If you want to train your own model, such as a part-of-speech tagger or text classifier, you will need to create a custom corpus to train on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79253ec-98e3-4093-9123-36b959d5bf15",
   "metadata": {},
   "source": [
    "***\n",
    "## ***<a id=\"custom-corpus\"></a>Setting up a custom corpus:***\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c176eeb-f735-42d6-9404-bbaa324a3e17",
   "metadata": {},
   "source": [
    "A **corpus** is a collection of text documents, and **corpora** is the plural of corpus. This comes from the Latin word for body; in this case, a body of text. So a custom corpus is really just a bunch of text files in a directory, often alongside many other directories of text files.\n",
    "\n",
    "**NLTK** defines a list of **data directories**, or **paths**, in `nltk.data.path`. Our custom corpora must be within one of these paths so it can be found by NLTK. In order to avoid conflict with the official data package, we will create a custom nltk_data directory in our home directory. The following is some Python code to create this directory and verify that it is in the list of known paths specified by `nltk.data.path:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14a7484f-4728-41f8-a85e-314926cbe32c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "#Creating the NLTK Data Path\n",
    "path = os.path.expanduser('~/nltk_data')\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "os.path.exists(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2211b6de-6cbe-4792-8049-1d10940ff385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Checking NLTK's Data Path\n",
    "import nltk.data\n",
    "path in nltk.data.path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eac986f-a29b-4fd2-bbbd-cd4608e2b11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nltk.data.path.append(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f63997-4e0e-4cf5-a0a5-0976479c2f06",
   "metadata": {},
   "source": [
    "Now, we can create a simple wordlist file and make sure it loads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bf72393-a25d-4fd9-9ac6-38aedc20e813",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus_path = os.path.join(path, 'corpora/cookbook')\n",
    "os.makedirs(corpus_path, exist_ok=True)\n",
    "with open(os.path.join(corpus_path, 'mywords.txt'), 'w') as f:\n",
    "    f.write('Your content here')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dccef17-e895-48b9-bbbb-d59c55f60fc6",
   "metadata": {},
   "source": [
    "Loading Custom NLTK Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e40eb3f-c390-45f9-8f9c-466678ab6705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Your content here'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.data.load('corpora/cookbook/mywords.txt', format='raw')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8299f298-a1b8-48bf-8465-1fff6d77dea1",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "## ***<a id=\"wordlist-corpus\"></a>Creating a wordlist corpus:***\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241db67e-2ba3-456a-8bab-501709ff55b5",
   "metadata": {},
   "source": [
    "The `WordListCorpusReader` class is one of the simplest **CorpusReader classes**. It provides access to a file containing a list of words, one word per line. \n",
    "\n",
    "We need to start by creating a word_list file. This could be a single column CSV file, or just a normal text file with one word per line. Let's create a file named wordlist that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ff11d67-7f3f-48da-adbf-f16edc203cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word list saved to word_list.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "words = [\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\", \"fig\", \"grape\"]\n",
    "\n",
    "file_name = \"word_list.txt\"\n",
    "\n",
    "with open(file_name, \"w\") as file:\n",
    "    for word in words:\n",
    "        file.write(word + \"\\n\")\n",
    "\n",
    "print(f\"Word list saved to {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0e9d04a-6c08-448d-a971-85aa558d16cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus.reader import WordListCorpusReader\n",
    "reader = WordListCorpusReader('.', [file_name])\n",
    "reader.words()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07cd8a8f-59f5-4169-a9b1-76fc89a09349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word_list.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reader.fileids()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee37abc-b56b-4c85-8b4b-650a9d5bd8e9",
   "metadata": {},
   "source": [
    "***How it works...***\n",
    "- The `WordListCorpusReader` class inherits from `CorpusReader`, which is a common base class for all corpus readers. The CorpusReader class does all the work of identifying which files to read, while `WordListCorpusReader` reads the files and tokenizes each line to produce a list of words.\n",
    "\n",
    "When you call the `words()` function, it calls **nltk.tokenize.line_tokenize()** on the raw file data, which you can access using the `raw()` function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a70a7353-fa40-44ed-90cd-75b8f7846ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apple\\r\\nbanana\\r\\ncherry\\r\\ndate\\r\\nelderberry\\r\\nfig\\r\\ngrape\\r\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reader.raw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bb3580a-f9a6-4b72-84de-8c12a8855193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import line_tokenize\n",
    "line_tokenize(reader.raw())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fb2427-6904-4290-8529-eabd11bc93c5",
   "metadata": {},
   "source": [
    "***Names wordlist corpus:***\n",
    "\n",
    "Another wordlist corpus that comes with NLTK is the `names corpus` that is shown in the following code. It contains two files: female.txt and male.txt, each containing a list of a few thousand common first names organized by gender as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14c68c17-a156-4559-a823-b9eb62f997f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['female.txt', 'male.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import names\n",
    "names.fileids()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d92e91cf-1d80-4e5a-8ef5-58b5494d8bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5001, 2943)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(names.words('female.txt')),len(names.words('male.txt'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a90210-c415-4868-a3ae-8f843497146d",
   "metadata": {},
   "source": [
    "***English words corpus:***\n",
    "\n",
    "NLTK also comes with a large list of English words. There's one file with 850 basic words, and another list with over 200,000 known English words, as shown in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c9b6fd8-7695-4029-a9eb-3fa5d8712536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['en', 'en-basic']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import words\n",
    "words.fileids()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba7a67c9-8cfd-4622-b776-9c27ae03b826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 235886)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(words.words('en-basic')),len(words.words('en'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19900722-6c5e-420a-b563-023678a8769d",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "## ***<a id=\"pos-tagged-corpus\"></a>Creating a part-of-speech tagged word corpus:***\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e339ade-03c9-4fcf-9ae9-7ee797df628f",
   "metadata": {},
   "source": [
    "**`art-of-speech`** tagging is the process of identifying the **part-of-speech** tag for a word. Most of \n",
    "the time, a tagger must first be trained on a training corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6766125-0462-41e1-bf7b-3f8b2179d9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw content of file: ['apple', 'banana', 'cherry', 'date', 'elderberry', ...]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus.reader import TaggedCorpusReader\n",
    "# Use TaggedCorpusReader to read the word_list.txt file\n",
    "corpus_root = \".\"  # Current directory\n",
    "file_pattern = r\"word_list\\.txt\"\n",
    "\n",
    "reader = TaggedCorpusReader(corpus_root, file_pattern)\n",
    "\n",
    "# Access raw data\n",
    "print(\"Raw content of file:\", reader.words())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "218f2474-c1e8-421c-bee7-47be7d211689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apple', None), ('banana', None), ('cherry', None), ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reader.tagged_words()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "734b79a8-e051-4940-bad1-fa709eb681ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['apple'], ['banana'], ['cherry'], ['date'], ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reader.sents()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d93494f9-3b9b-4147-847f-3c7ce820a9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('apple', None)], [('banana', None)], ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reader.tagged_sents()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2658dd2-cfda-46ff-ba34-c7714ffa12af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['apple'], ['banana'], ['cherry'], ['date'], ['elderberry'], ['fig'], ['grape']]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reader.paras()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56f54d4a-725c-4fda-90fd-5194f5d6594d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[('apple', None)], [('banana', None)], [('cherry', None)], [('date', None)], [('elderberry', None)], [('fig', None)], [('grape', None)]]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reader.tagged_paras()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581e2c59-8c2f-42f5-83d2-5665c9327ab3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "***\n",
    "## ***<a id=\"chunked-corpus\"></a>Creating a chunked phrase corpus:***\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12018ba2-e3ef-44b4-a686-0f1475261177",
   "metadata": {},
   "source": [
    "A **chunk** is a short phrase within a **sentence**. If you remember sentence diagrams from grade school, they were a tree-like representation of phrases within a sentence. This is exactly what chunks are. Chunking, also called shallow parsing, groups words into meaningful phrases like noun phrases (NPs), verb phrases (VPs), and prepositional phrases (PPs). This is useful for named entity recognition (NER), question answering, and text classification.\n",
    "\n",
    "***Steps to Create a Chunked Phrase Corpus:***\n",
    "\n",
    " - `Load the Text` – Define or import textual data.\n",
    " - `Tokenization & POS Tagging` – Split text into words and assign POS tags.\n",
    " - `Define a Chunk Grammar` – Use Regular Expressions (Regex) patterns to extract phrases.\n",
    " - `Chunking` – Apply the grammar to extract noun/verb/prepositional phrases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7ae742d-78de-49ec-abf8-409c7de56a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP A/DT chunk/NN)\n",
      "  (VP is/VBZ (NP a/DT short/JJ phrase/NN))\n",
      "  (PP within/IN (NP a/DT sentence/NN))\n",
      "  ./.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\DELL/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\DELL/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, RegexpParser\n",
    "\n",
    "# Download necessary data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Sample text\n",
    "text = \"A chunk is a short phrase within a sentence.\"\n",
    "\n",
    "# Tokenization and POS Tagging\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Define Chunk Grammar (Noun Phrases: NP, Verb Phrases: VP)\n",
    "chunk_grammar = r\"\"\"\n",
    "    NP: {<DT>?<JJ>*<NN>}      # Noun phrase: Determiner (optional) + Adjective(s) + Noun\n",
    "    VP: {<VB.*><NP|PP>*}      # Verb phrase: Verb + Noun Phrase or Prepositional Phrase\n",
    "    PP: {<IN><NP>}            # Prepositional phrase: Preposition + Noun Phrase\n",
    "\"\"\"\n",
    "\n",
    "# Create a parser\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "# Apply chunking\n",
    "chunk_tree = chunk_parser.parse(pos_tags)\n",
    "\n",
    "# Print chunked structure\n",
    "print(chunk_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2d32a7-bfd1-4b7e-a2cf-3ebbd974cfb0",
   "metadata": {},
   "source": [
    "***We can extract phrases directly from the chunk tree.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "845a36b3-42e6-4c8d-a089-e192874475d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun Phrases: ['A chunk', 'a short phrase', 'a sentence']\n",
      "Verb Phrases: ['is a short phrase']\n",
      "Prepositional Phrases: ['within a sentence']\n"
     ]
    }
   ],
   "source": [
    "# Extract noun phrases (NPs) from the chunk tree\n",
    "def extract_phrases(tree, label):\n",
    "    phrases = []\n",
    "    for subtree in tree.subtrees(filter=lambda t: t.label() == label):\n",
    "        phrase = \" \".join(word for word, pos in subtree.leaves())\n",
    "        phrases.append(phrase)\n",
    "    return phrases\n",
    "\n",
    "# Extract different phrase types\n",
    "noun_phrases = extract_phrases(chunk_tree, \"NP\")\n",
    "verb_phrases = extract_phrases(chunk_tree, \"VP\")\n",
    "prepositional_phrases = extract_phrases(chunk_tree, \"PP\")\n",
    "\n",
    "# Print results\n",
    "print(\"Noun Phrases:\", noun_phrases)\n",
    "print(\"Verb Phrases:\", verb_phrases)\n",
    "print(\"Prepositional Phrases:\", prepositional_phrases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248d8b96-34a6-4282-a46e-7df4e962228f",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "## ***<a id=\"categorized-text-corpus\"></a>Creating a categorized text corpus:***\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5165c373-4c23-4860-b740-96857d4e5660",
   "metadata": {},
   "source": [
    "A **categorized text corpus** is a **collection of documents** that are labeled according to their content or subject. It is useful for tasks like text classification, sentiment analysis, and topic modeling.\n",
    "\n",
    "***Steps to Create a Categorized Text Corpus***\n",
    "- `Gather Text Data` – Collect a set of documents or text data.\n",
    "- `Define Categories` – Decide on the categories you want to label your data with.\n",
    "- `Preprocess the Text` – Clean and prepare the text (e.g., tokenization, lowercasing).\n",
    "- `Label the Text` – Assign categories to each document.\n",
    "\n",
    "***We will create a small example with text data manually labeled:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab8fff78-a564-4db4-954a-d5d75fa7d91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The soccer match ended in a 2-1 victory for th...</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The president announced a new economic policy ...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A new smartphone with cutting-edge technology ...</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The basketball game was intense, with both tea...</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The government is focusing on digital infrastr...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A major tech company unveiled its latest AI in...</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    category\n",
       "0  The soccer match ended in a 2-1 victory for th...      Sports\n",
       "1  The president announced a new economic policy ...    Politics\n",
       "2  A new smartphone with cutting-edge technology ...  Technology\n",
       "3  The basketball game was intense, with both tea...      Sports\n",
       "4  The government is focusing on digital infrastr...    Politics\n",
       "5  A major tech company unveiled its latest AI in...  Technology"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample text data (articles)\n",
    "data = [\n",
    "    {\"text\": \"The soccer match ended in a 2-1 victory for the home team.\", \"category\": \"Sports\"},\n",
    "    {\"text\": \"The president announced a new economic policy to improve the job market.\", \"category\": \"Politics\"},\n",
    "    {\"text\": \"A new smartphone with cutting-edge technology was released today.\", \"category\": \"Technology\"},\n",
    "    {\"text\": \"The basketball game was intense, with both teams scoring high.\", \"category\": \"Sports\"},\n",
    "    {\"text\": \"The government is focusing on digital infrastructure and public services.\", \"category\": \"Politics\"},\n",
    "    {\"text\": \"A major tech company unveiled its latest AI innovations.\", \"category\": \"Technology\"}\n",
    "]\n",
    "\n",
    "# Convert the data into a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c757eb-5bd0-4a73-9e79-31ef58e0ccc7",
   "metadata": {},
   "source": [
    "***We can tokenize the text and convert it to lowercase to standardize it for further analysis:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d543fb8-b6fb-4052-8fae-41a19a0a6981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\DELL/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The soccer match ended in a 2-1 victory for th...</td>\n",
       "      <td>the soccer match ended in a victory for the ho...</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The president announced a new economic policy ...</td>\n",
       "      <td>the president announced a new economic policy ...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A new smartphone with cutting-edge technology ...</td>\n",
       "      <td>a new smartphone with technology was released ...</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The basketball game was intense, with both tea...</td>\n",
       "      <td>the basketball game was intense with both team...</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The government is focusing on digital infrastr...</td>\n",
       "      <td>the government is focusing on digital infrastr...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A major tech company unveiled its latest AI in...</td>\n",
       "      <td>a major tech company unveiled its latest ai in...</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The soccer match ended in a 2-1 victory for th...   \n",
       "1  The president announced a new economic policy ...   \n",
       "2  A new smartphone with cutting-edge technology ...   \n",
       "3  The basketball game was intense, with both tea...   \n",
       "4  The government is focusing on digital infrastr...   \n",
       "5  A major tech company unveiled its latest AI in...   \n",
       "\n",
       "                                      processed_text    category  \n",
       "0  the soccer match ended in a victory for the ho...      Sports  \n",
       "1  the president announced a new economic policy ...    Politics  \n",
       "2  a new smartphone with technology was released ...  Technology  \n",
       "3  the basketball game was intense with both team...      Sports  \n",
       "4  the government is focusing on digital infrastr...    Politics  \n",
       "5  a major tech company unveiled its latest ai in...  Technology  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Tokenization and conversion to lowercase\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalnum()]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the 'text' column\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Display the preprocessed DataFrame\n",
    "df[['text', 'processed_text', 'category']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23daff5f-fe6e-4db6-8fbc-a16f7befdbb8",
   "metadata": {},
   "source": [
    "***You can store the categorized corpus in CSV or JSON format for future use:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3737963-8559-453f-a683-5e917106fe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Save as CSV\n",
    "df.to_csv(\"categorized_corpus.csv\", index=False)\n",
    "\n",
    "#Save as JSON\n",
    "df.to_json(\"categorized_corpus.json\", orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4910cbb-980a-41ad-9559-2910be3da740",
   "metadata": {},
   "source": [
    "\n",
    "*** \n",
    "## ***<a id=\"categorized-chunk-corpus\"></a>Creating a categorized chunk corpus reader:***\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b1ce39-7840-43ac-93a5-108fd2c5030f",
   "metadata": {},
   "source": [
    "A **categorized chunk corpus** refers to a collection of chunked texts that are labeled by category, such as subject, genre, or any custom category. It’s commonly used in NLP tasks like text classification, part-of-speech tagging, and named entity recognition (NER).\n",
    "\n",
    "We can create a categorized chunk corpus where the text is chunked into phrases and categorized based on a specific label. This corpus can then be read and processed for machine learning or text analysis tasks.\n",
    "\n",
    "***Steps to Create a Categorized Chunk Corpus Reader:***\n",
    " - `Prepare Your Text and Categories:` First, gather the raw text and assign categories (labels) to them.\n",
    " - `Preprocess the Text:` Tokenize and apply POS tagging.\n",
    " - `Chunk the Text:` Use a chunking grammar to identify phrases.\n",
    " - `Label Each Chunk:` Add a label or category to each chunked text.\n",
    " - `Create the Corpus Reader:` Write a custom reader to load and process the chunked text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "352ce8f3-c591-4723-8328-304a987f97aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\DELL/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\DELL/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, RegexpParser\n",
    "\n",
    "# Download necessary data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61462b79-45d2-40e3-8f3b-4944620c20a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample text and category\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "category = \"Sports\"  # Label the text as 'Sports'\n",
    "\n",
    "# Tokenization and POS Tagging\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c0e35e7-d14b-4ff0-a85c-3a4284414c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define Chunk Grammar (NP, VP, PP)\n",
    "chunk_grammar = r\"\"\"\n",
    "    NP: {<DT>?<JJ>*<NN>}      # Noun Phrase: Determiner (optional) + Adjective(s) + Noun\n",
    "    VP: {<VB.*><NP|PP>*}       # Verb Phrase: Verb + Noun Phrase or Prepositional Phrase\n",
    "    PP: {<IN><NP>}             # Prepositional Phrase: Preposition + Noun Phrase\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2235885-6002-43d4-9afa-3b86d05c1c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a parser\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "# Apply chunking\n",
    "chunk_tree = chunk_parser.parse(pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34dbe285-1f92-4211-9181-338904a1befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract phrases from the chunk tree\n",
    "def extract_phrases(tree, label):\n",
    "    phrases = []\n",
    "    for subtree in tree.subtrees(filter=lambda t: t.label() == label):\n",
    "        phrase = \" \".join(word for word, pos in subtree.leaves())\n",
    "        phrases.append(phrase)\n",
    "    return phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "080216a6-ea2a-4c57-942e-90d8be8aa1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract phrases\n",
    "noun_phrases = extract_phrases(chunk_tree, \"NP\")\n",
    "verb_phrases = extract_phrases(chunk_tree, \"VP\")\n",
    "prepositional_phrases = extract_phrases(chunk_tree, \"PP\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70183e1f-b749-40d3-a2ca-5e2b79900a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Store chunked text with categories\n",
    "chunked_data = {\n",
    "    \"category\": category,\n",
    "    \"noun_phrases\": noun_phrases,\n",
    "    \"verb_phrases\": verb_phrases,\n",
    "    \"prepositional_phrases\": prepositional_phrases\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2269017-f9c1-4082-aa18-af8a7e4a0b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'category': 'Sports', 'noun_phrases': ['The quick brown', 'fox', 'the lazy dog'], 'verb_phrases': ['jumps'], 'prepositional_phrases': ['over the lazy dog']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(chunked_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecb69e7-38ed-4ef6-b337-1e5a0bdf4e23",
   "metadata": {},
   "source": [
    "\n",
    "*** \n",
    "## ***<a id=\"lazy-loading\"></a>Lazy corpus loading:***\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16244ecc-bb8e-4efb-9cb5-53c2a212efac",
   "metadata": {},
   "source": [
    "Lazy corpus loading refers to the technique of reading and processing data on demand, rather than loading everything into memory all at once. This is especially useful when dealing with large datasets that might not fit entirely into memory, allowing for more efficient resource management.\n",
    "\n",
    "Loading a corpus reader can be an expensive operation due to the number of files, file sizes, and various initialization tasks. And while you'll often want to specify a corpus reader in a common module, you don't always need to access it right away. To speed up module import time when a corpus reader is defined, NLTK provides a LazyCorpusLoader class that can transform itself into your actual corpus reader as soon as you need it. This way, you can define a corpus reader in a common module without it slowing down module loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "62d89bf7-ec48-432d-982d-a75965a5ca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus.util import LazyCorpusLoader\n",
    "from nltk.corpus.reader import WordListCorpusReader\n",
    "reader = LazyCorpusLoader('cookbook', WordListCorpusReader,['wordlist'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1dd8ea7f-cd4c-44fe-8be3-5ce5a11765bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "isinstance(reader, LazyCorpusLoader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3da90f5-12d0-43c7-8b6e-86d6c0f7d232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "isinstance(reader, WordListCorpusReader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0352ff-861d-4f74-94fc-92c829837eed",
   "metadata": {},
   "source": [
    "\n",
    "*** \n",
    "## ***<a id=\"custom-corpus-view\"></a>Creating a custom corpus view:***\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66670bd-09fa-4094-9c66-d3a21adf75ac",
   "metadata": {},
   "source": [
    "A **corpus view** is a class wrapper around a corpus file that reads in blocks of tokens as needed. Its purpose is to provide a view into a file without reading the whole file at once (since corpus files can often be quite large). If the corpus readers included by NLTK already meet all your needs, then you do not have to know anything about corpus views. But, if you have a custom file format that needs special handling, this recipe will show you how to create and use a custom corpus view. The main corpus view class is StreamBackedCorpusView, which opens a single file as a stream, and maintains an internal cache of blocks it has read.\n",
    "\n",
    "We will start with the simple case of a plain text file with a heading that should be ignored by the corpus reader. Let's make a file called heading_text.txt that looks like this:\n",
    "$$\n",
    "A simple heading.\n",
    "$$\n",
    "$$\n",
    "Here is the actual text for the corpus.\n",
    "$$\n",
    "$$\n",
    "Paragraphs are split by blanklines.\n",
    "$$\n",
    "$$\n",
    "This is the 3rd paragraph.\n",
    "$$\n",
    "\n",
    "Normally, we would use the `PlaintextCorpusReader` class, but by default it will treat A simple heading as the first paragraph. To ignore this heading, we need to subclass the `PlaintextCorpusReader` class so we can override its CorpusView class variable with our \n",
    "own `StreamBackedCorpusView` subclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fe6233dd-ab68-445b-a0e2-6720fa7d9fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus.reader import PlaintextCorpusReader\n",
    "from nltk.corpus.reader.util import StreamBackedCorpusView\n",
    "\n",
    "class IgnoreHeadingCorpusView(StreamBackedCorpusView):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        StreamBackedCorpusView.__init__(self, *args, **kwargs)\n",
    "        # open self._stream\n",
    "        self._open()\n",
    "        # skip the heading block\n",
    "        self.read_block(self._stream)\n",
    "        # reset the start position to the current position in the stream\n",
    "        self._filepos = [self._stream.tell()]\n",
    "\n",
    "class IgnoreHeadingCorpusReader(PlaintextCorpusReader):\n",
    "    CorpusView = IgnoreHeadingCorpusView\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9736881-d932-49dc-9b9f-4edc9d109595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus.reader import PlaintextCorpusReader\n",
    "plain = PlaintextCorpusReader('.', ['heading_text.txt'])\n",
    "len(plain.paras())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aefbc9cb-da9c-46b5-8abf-32a38952656f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reader = IgnoreHeadingCorpusReader('.', ['heading_text.txt'])\n",
    "len(reader.paras())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0603ea76-6d77-4a40-bd4a-e9d62b1e97d9",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "## ***<a id=\"mongodb-corpus\"></a>Creating a MongoDB-backed corpus reader:***\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f154b98-c1b3-46df-8033-8df77ec76c45",
   "metadata": {},
   "source": [
    "All the corpus readers we have dealt with so far have been file-based. That is in part due to the design of the CorpusReader base class, and also the assumption that most corpus data will be in text files. However, sometimes you will have a bunch of data stored in a database that you want to access and use just like a text file corpus. In this recipe, we will cover the case where you have documents in **MongoDB**, and you want to use a particular field of each document as your block of text.\n",
    "\n",
    "MongoDB is a document-oriented database that has become a popular alternative to relational databases such as MySQL.  You will also need to install PyMongo, a Python driver for MongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ddb8056-a5e5-4eb9-94af-723c98a84566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing dependencies of celery: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    pytz (>dev)\n",
      "         ~^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pymongo\n",
      "  Downloading pymongo-10.10.10.10-cp311-cp311-win_amd64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from pymongo) (2.6.1)\n",
      "Downloading pymongo-10.10.10.10-cp311-cp311-win_amd64.whl (831 kB)\n",
      "   ---------------------------------------- 0.0/831.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/831.7 kB ? eta -:--:--\n",
      "   ------------ --------------------------- 262.1/831.7 kB ? eta -:--:--\n",
      "   ------------ --------------------------- 262.1/831.7 kB ? eta -:--:--\n",
      "   ------------ --------------------------- 262.1/831.7 kB ? eta -:--:--\n",
      "   -------------------------------------- 831.7/831.7 kB 701.4 kB/s eta 0:00:00\n",
      "Installing collected packages: pymongo\n",
      "Successfully installed pymongo-10.10.10.10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install pymongo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "452f2fe6-6ff4-479f-995f-649e3a9abb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pymongo\n",
    "from nltk.data import load\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.util import AbstractLazySequence, LazyMap, LazyConcatenation\n",
    "\n",
    "class MongoDBLazySequence(AbstractLazySequence):\n",
    "    \n",
    "    def __init__(self, host='localhost', port=27017, db='test',  \n",
    "        collection='corpus', field='text'):\n",
    "        self.conn = pymongo.MongoClient(host, port)\n",
    "        self.collection = self.conn[db][collection]\n",
    "        self.field = field\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.collection.count_documents({})  # Fixed count() deprecation\n",
    "        \n",
    "    def iterate_from(self, start):\n",
    "        f = lambda d: d.get(self.field, '')\n",
    "        return iter(LazyMap(f, self.collection.find({}, {self.field: 1}, skip=start)))  # Fixed find()\n",
    "\n",
    "class MongoDBCorpusReader:\n",
    "    def __init__(self, word_tokenizer=TreebankWordTokenizer(),\n",
    "        sent_tokenizer=load('tokenizers/punkt/english.pickle'), **kwargs):\n",
    "        self._seq = MongoDBLazySequence(**kwargs)\n",
    "        self._word_tokenize = word_tokenizer.tokenize\n",
    "        self._sent_tokenize = sent_tokenizer.tokenize\n",
    "        \n",
    "    def text(self):\n",
    "        return self._seq\n",
    "        \n",
    "    def words(self):\n",
    "        return LazyConcatenation(LazyMap(self._word_tokenize, self.text()))\n",
    "        \n",
    "    def sents(self):\n",
    "        return LazyConcatenation(LazyMap(self._sent_tokenize, self.text()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bb2f37-e316-4c41-850b-55e5f664d1d9",
   "metadata": {},
   "source": [
    "The `AbstractLazySequence` class is an abstract class that provides read-only, on-demand iteration. Subclasses must implement the **__len__()** and iterate_from(start) methods, while it provides the rest of the list and iterator emulation methods. By creating the `MongoDBLazySequence` subclass as our view, we can iterate over documents in the MongoDB collection on demand, without keeping all the documents in memory. The `LazyMap`class is a lazy version of Python's built-in map() function, and is used in iterate_from() to transform the document into the specific field that we're interested in. It's also a subclass of `AbstractLazySequence`.\n",
    "\n",
    "The `MongoDBCorpusReade`r class creates an internal instance of `MongoDBLazySequence` for iteration, then defines the word and sentence tokenization methods. The text() method simply returns the instance of MongoDBLazySequence, which results in a lazily evaluated list of each text field. The words() method uses LazyMap and LazyConcatenation to return a lazily evaluated list of all words, while the sents() method does the same for sentences. The sent_tokenizer is loaded on demand with LazyLoader, which is a wrapper around nltk.data.load(), analogous to LazyCorpusLoader. The LazyConcatentation class is a subclass of `AbstractLazySequence`too, and produces a flat list from a given list of lists (each list may also be lazy). In our case, we're concatenating the results of LazyMap to ensure we don't return nested lists.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9aabf5-dbb3-45e4-8ac7-7aa4c5d9dc6b",
   "metadata": {},
   "source": [
    "All of the **parameters** are configurable. For example, if you had a db named website, with a collection named comments, whose documents had a field called comment, you could create a MongoDBCorpusReader class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e44bf01d-5766-4a98-962c-7bb3d5f6384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reader = MongoDBCorpusReader(db='website',collection='comments', field='comment')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5c5947-27c2-42e4-a091-c333bc449787",
   "metadata": {},
   "source": [
    "You can also pass in custom instances for **word_tokenizer** and **sent_tokenizer**, as long as the objects implement the nltk.tokenize.TokenizerI interface by providing a tokenize(text) method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1c18b8-6b96-4e9f-90af-1bca71d5a53a",
   "metadata": {},
   "source": [
    "\n",
    "***       \n",
    "## ***<a id=\"file-locking\"></a>Corpus editing with file No Locking:***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3331e7ef-6445-4e99-bd5e-e2536b94cd82",
   "metadata": {},
   "source": [
    "Corpus readers and views are all read-only, but there will be times when you want to add to or edit the corpus files. However, modifying a corpus file while other processes are using it, such as through a corpus reader, can lead to dangerous undefined ehavior. This is where file locking comes in handy.\n",
    "\n",
    "When working with corpus editing, ensuring file integrity is crucial, especially in a multi-threaded or concurrent environment. Using file locking prevents multiple processes from modifying the file simultaneously, reducing data corruption risks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4b80dacc-f757-4867-92cc-a8214ff1eddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Content:\n",
      " This is the original corpus content.\n",
      "\n",
      "Updated Corpus Successfully!\n",
      "\n",
      "Final Corpus Words: ['This', 'is', 'the', 'updated', 'corpus', 'content', ...]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import msvcrt\n",
    "class LockedCorpusEditor:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def edit_corpus(self, new_text):\n",
    "        \"\"\"Edit the corpus file without explicit locking.\"\"\"\n",
    "        with open(self.file_path, 'r+', encoding='utf-8') as file:\n",
    "            # Read the original content\n",
    "            original_content = file.read()\n",
    "            print(\"Original Content:\\n\", original_content)\n",
    "\n",
    "            # Move the pointer to the beginning and overwrite the file\n",
    "            file.seek(0)\n",
    "            file.write(new_text)\n",
    "            file.truncate()  # Remove leftover text from previous content\n",
    "            \n",
    "            print(\"\\nUpdated Corpus Successfully!\")\n",
    "\n",
    "# Example Usage\n",
    "corpus_file = \"sample_corpus.txt\"\n",
    "\n",
    "# Creating a sample corpus file\n",
    "with open(corpus_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"This is the original corpus content.\")\n",
    "\n",
    "# Editing the corpus safely\n",
    "editor = LockedCorpusEditor(corpus_file)\n",
    "editor.edit_corpus(\"This is the updated corpus content.\")\n",
    "\n",
    "# Loading the edited corpus with NLTK\n",
    "from nltk.corpus.reader import PlaintextCorpusReader\n",
    "reader = PlaintextCorpusReader('.', [corpus_file])\n",
    "print(\"\\nFinal Corpus Words:\", reader.words(corpus_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1321cf8c-846b-4c58-8d03-95f48b5cf353",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
